# ü§ñ API de RAG para Consultar Reglas de F√∫tbol

Este es un proyecto personal enfocado en explorar el stack moderno de IA para construir un sistema de "Preguntas y Respuestas" (Q&A). El objetivo principal fue aprender e implementar una arquitectura **RAG (Retrieval-Augmented Generation)** y exponerla a trav√©s de una **API REST**.

El proyecto responde preguntas sobre el reglamento oficial de f√∫tbol ("Laws of the Game 2025/26").

## üí° Inspiraci√≥n y Referencias

La l√≥gica base para la implementaci√≥n del RAG fue adaptada del excelente tutorial de **Pixegami**. Este proyecto expande esa base al:
1.  Cambiar el modelo de embeddings (de OpenAI a HuggingFace local).
2.  Cambiar el modelo de chat (de OpenAI a Google Gemini).
3.  "Envolver" toda la l√≥gica de consulta en una API de FastAPI.

* **Video Tutorial Original:** [RAG + Langchain Python Project: Easy AI/Chat For Your Docs](https://www.youtube.com/watch/tcqEUSNCn8I)
* **Repositorio Original:** [github.com/pixegami/langchain-rag-tutorial](https://github.com/pixegami/langchain-rag-tutorial)

---

## üìã Descripci√≥n del Proyecto

Este proyecto es una API REST que responde preguntas sobre el reglamento de f√∫tbol. Utiliza una arquitectura RAG para asegurar que las respuestas se basen √∫nicamente en el contenido del documento.

### Arquitectura
1.  **Carga de Datos (`create_db.py`):** Un script de Python lee un PDF (`Laws of the Game 2025_26.pdf`), lo divide en "chunks" (trozos) de texto y los procesa.
2.  **Embeddings (Local):** Se utiliza un modelo de embeddings de HuggingFace (`all-MiniLM-L6-v2`) para convertir cada chunk de texto en vectores num√©ricos. Esto se ejecuta localmente y es gratuito.
3.  **Base de Datos Vectorial (`Chroma`):** Los vectores se almacenan en una base de datos vectorial local (ChromaDB) para permitir b√∫squedas de similitud r√°pidas.
4.  **API REST (`main.py`):** Una API construida con **FastAPI** expone un endpoint `/preguntar`.
5.  **Proceso RAG (en la API):**
    * Cuando la API recibe una pregunta (ej. *"What is an offside?"*), la convierte en un vector usando el mismo modelo de embeddings.
    * Busca en la base de datos `Chroma` los chunks de texto m√°s similares a la pregunta.
    * Construye un prompt que incluye la pregunta del usuario y el contexto encontrado.
    * Env√≠a el prompt a la API de **Google Gemini** (`gemini-flash-latest`) para generar una respuesta.
    * La API devuelve la respuesta y las fuentes (el PDF) en un formato JSON.

---

## üöÄ C√≥mo ejecutar el proyecto

Este proyecto est√° dividido en dos partes: crear la base de datos y correr la API.

### Requisitos

Aseg√∫rate de tener un archivo `.env` en la ra√≠z con tu clave de API de Google:
GOOGLE_API_KEY="AIzaSy..."


Luego, instala las dependencias (se recomienda usar un entorno virtual):
```bash
pip install -r requirements.txt
1. Crear la Base de Datos
(Este paso solo se hace una vez)

Bash

python create_db.py
Esto leer√° el PDF en la carpeta /data y crear√° la base de datos en la carpeta /chroma.

2. Correr la API
Bash

python main.py
El servidor se iniciar√° en http://127.0.0.1:8000.

3. Probar la API
Puedes usar la documentaci√≥n interactiva de FastAPI que se genera autom√°ticamente.

Abre tu navegador y ve a: http://127.0.0.1:8000/docs

Haz clic en el endpoint /preguntar y luego en "Try it out".

Escribe tu pregunta en el "Request body":

JSON

{
  "texto": "What is an offside offence?"
}
Presiona "Execute" y ver√°s la respuesta en JSON.